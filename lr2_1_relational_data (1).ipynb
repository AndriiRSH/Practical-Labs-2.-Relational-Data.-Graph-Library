{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ju_iBe2hS5UC"
   },
   "outputs": [],
   "source": [
    "# Colab only: Run this cell to download/install packages\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1fLqy4p1S3Xp"
   },
   "source": [
    "# Relational Data and Visualization\n",
    "In this problem, you will be analyzing the Twitter data we extracted using [this](https://developer.twitter.com/en/docs/twitter-api) api, in 2016. The data consists of Twitter users (with unique handles) and their attributes (e.g., number of followers), some recent tweets posted by them with attributes (e.g., time stamp, number of retweets), and the follow relationship between the users. These are available in the three (gzipped) CSV files provided to you:\n",
    "- users.csv.gz - users, user attributes\n",
    "- edges.csv.gz - follow edges (directed, an edge from A to B means A follows B or B is a friend of A)\n",
    "- tweets.csv.gz - tweets posted by the users along with its attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 372,
     "status": "ok",
     "timestamp": 1665480086669,
     "user": {
      "displayName": "Галина Василівна Мельник",
      "userId": "15364269542359895576"
     },
     "user_tz": -180
    },
    "id": "PrRLhcXvS3Xv"
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G02yqCyQS3Xx"
   },
   "source": [
    "## Q1. Relational Data\n",
    "This question will guide you through loading Twitter data into an in-memory SQLite database and running some basic queries on it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HvmTfWJhS3Xx"
   },
   "source": [
    "### Q1. Task A: Load Twitter data into SQLite database\n",
    "Your first task is to use the (gzipped) csv and sqlite3 python packages to load the three csv files we give you as relations (or tables) into an SQLite in-memory database.\n",
    "\n",
    "Loading the data from (gzipped) csv file into the database involves the following steps:\n",
    "1. Identify the schema of the table (for this problem, you will only need TEXT and INTEGER attribute types)\n",
    "2. Create a table with the identified schema\n",
    "3. Load the contents of csv in memory\n",
    "4. Insert every row of csv file as a record in the table\n",
    "\n",
    "You can refer to [sqlite3 documentation](https://docs.python.org/2/library/sqlite3.html) and the class lecture for steps 2 and 4. For step 3 you can use pandas. Be sure to name your tables `users`, `edges`, and `tweets`. \n",
    "\n",
    "Make sure to commit (the equivalent of Ctrl+S for databases) any changes you make to the database. [This](https://www.techopedia.com/definition/16/commit) page should give you an idea about why commit is essential.\n",
    "\n",
    "Don't decompress the `.gz` files - we do that while reading them, i.e., pass the `*.csv.gz` filenames directly into a pandas function. This is common practice when dealing with large amounts of text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "geThLml6S3Xy"
   },
   "outputs": [],
   "source": [
    "def load_twitter_data_sqlite3(conn):\n",
    "    \"\"\"Load twitter data into an SQLite in-memory database.\"\"\"\n",
    "    # Define the table names and corresponding CSV files\n",
    "    data_files = {\n",
    "        'users': 'users.csv.gz',\n",
    "        'edges': 'edges.csv.gz',\n",
    "        'tweets': 'tweets.csv.gz',\n",
    "    }\n",
    "\n",
    "    for table_name, file_name in data_files.items():\n",
    "        # Read the gzipped CSV file into a pandas DataFrame\n",
    "        with gzip.open(file_name, 'rt') as f:\n",
    "            df = pd.read_csv(f)\n",
    "        \n",
    "        # Infer the schema from the DataFrame and map to SQLite types\n",
    "        schema = \", \".join(\n",
    "            f\"{col} {'INTEGER' if pd.api.types.is_integer_dtype(df[col]) else 'TEXT'}\"\n",
    "            for col in df.columns\n",
    "        )\n",
    "\n",
    "        # Create the table in SQLite\n",
    "        create_table_query = f\"CREATE TABLE IF NOT EXISTS {table_name} ({schema});\"\n",
    "        conn.execute(create_table_query)\n",
    "\n",
    "        # Insert the data into the table\n",
    "        df.to_sql(table_name, conn, if_exists='replace', index=False)\n",
    "\n",
    "    # Commit changes\n",
    "    conn.commit()\n",
    "\n",
    "    return conn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DjVsIbd6S3X5"
   },
   "source": [
    "### Q1. Task B: Trending tweets in a topic\n",
    "Twitter is regarded as an invaluable source of valuable information. Hence, one of the favorite tasks of data miners is to analyze the trending tweets in a given topic.\n",
    "\n",
    "This task requires you to retrieve the top N most trending tweets (in descending order of trending_score) about a given topic (which is a list of keywords). The following information may be useful:\n",
    "\n",
    "- A tweet is said to be about a given topic if it contains any of the given topical phrases/keywords.\n",
    "- We will use the following simple trending_score: retweet_count + favorite_count. Tweets with higher trending_score must be ranked before the ones with lower trending_score.\n",
    "- Your result must contain unique tweets. If a tweet text occurs multiple times, display it only once with its highest trending_score.\n",
    "- Break ties by sorting the tweets in alphabetical order.\n",
    "\n",
    "The output schema should be as follows:\n",
    "\n",
    "|tweet (TEXT)| trending_score (INTEGER) |\n",
    "| :--- |:--- |\n",
    "| | |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7NfzuT82S3X5"
   },
   "outputs": [],
   "source": [
    "def trending_tweets(cursor, topical_phrases, N):\n",
    "    \"\"\"Retrieve the top N trending tweets about a given topic.\"\"\"\n",
    "    # Prepare the query for filtering topical phrases\n",
    "    topical_filter = \" OR \".join([f\"tweet LIKE '%{phrase}%'\" for phrase in topical_phrases])\n",
    "\n",
    "    # SQL query to retrieve trending tweets\n",
    "    query = f\"\"\"\n",
    "        SELECT \n",
    "            tweet, \n",
    "            MAX(retweet_count + favorite_count) AS trending_score\n",
    "        FROM tweets\n",
    "        WHERE {topical_filter}\n",
    "        GROUP BY tweet\n",
    "        ORDER BY trending_score DESC, tweet ASC\n",
    "        LIMIT {N};\n",
    "    \"\"\"\n",
    "\n",
    "    # Execute the query and return results\n",
    "    results = cursor.execute(query)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0KRUuSlRS3X9"
   },
   "source": [
    "### Q1. Task C: Tweet recommendation\n",
    "How does Twitter go about populating the feed for a user? While Twitter may compile models to do this, in this task, we will use a Simple Tweet Recommender (STR), which recommends a user's tweets to all users who follow them (without checking for possible duplicates; i.e., STR may recommend the same tweet twice if two of a user's friends have posted it).\n",
    "\n",
    "In this task, you will write a query to determine the number of tweets recommended to each user. Use only the snapshot of edges and tweets we have provided to do the recommendation. Report the results on the users present in the users table. (Hint: The number of records in your output should match that in the \"users\" table.) The order of results does not matter.\n",
    "\n",
    "The output schema should be:\n",
    "\n",
    "|screen_name (TEXT)| num_tweets (INTEGER) |\n",
    "| :--- |:--- |\n",
    "| | | |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4VfuPAnVS3X9"
   },
   "outputs": [],
   "source": [
    "def num_tweets_in_feed(cursor):\n",
    "    \"\"\"Retrieve the number of tweets STR recommends to each Twitter user.\"\"\"\n",
    "    # SQL query to calculate number of recommended tweets per user\n",
    "    query = \"\"\"\n",
    "        SELECT \n",
    "            u.screen_name AS screen_name,\n",
    "            COALESCE(SUM(t.tweet_count), 0) AS num_tweets\n",
    "        FROM users u\n",
    "        LEFT JOIN edges e ON u.screen_name = e.to_user\n",
    "        LEFT JOIN (\n",
    "            SELECT from_user, COUNT(*) AS tweet_count\n",
    "            FROM tweets\n",
    "            GROUP BY from_user\n",
    "        ) t ON e.from_user = t.from_user\n",
    "        GROUP BY u.screen_name;\n",
    "    \"\"\"\n",
    "\n",
    "    # Execute the query and return results\n",
    "    return cursor.execute(query)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OUb0m-pmS3X_"
   },
   "source": [
    "## Q2. Visualization\n",
    "In this question, you will load all data into pandas dataframes and analyze (and visualize!) some interesting trends using [matplotlib](http://matplotlib.org) python package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "905gmdXtS3X_"
   },
   "source": [
    "### Q2. Task A: Load Twitter data using pandas \n",
    "Fill in the following method stub and return the data frames for users, edges and tweets.\n",
    "\n",
    "Pandas will treat missing values as NaNs by default. However, for this assignment, you should treat missing values (i.e., empty strings in the csv files) as empty strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k3DSBSs6S3YA"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gzip\n",
    "\n",
    "def load_twitter_data_pandas():\n",
    "    \"\"\"Load Twitter data from gzipped CSV files into pandas DataFrames.\"\"\"\n",
    "    # Define file paths\n",
    "    files = {\n",
    "        'users': 'users.csv.gz',\n",
    "        'edges': 'edges.csv.gz',\n",
    "        'tweets': 'tweets.csv.gz',\n",
    "    }\n",
    "    \n",
    "    # Load each file into a DataFrame with missing values treated as empty strings\n",
    "    users_df = pd.read_csv(gzip.open(files['users'], 'rt'), keep_default_na=False)\n",
    "    edges_df = pd.read_csv(gzip.open(files['edges'], 'rt'), keep_default_na=False)\n",
    "    tweets_df = pd.read_csv(gzip.open(files['tweets'], 'rt'), keep_default_na=False)\n",
    "    \n",
    "    return users_df, edges_df, tweets_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J97890KvS3YB"
   },
   "source": [
    "### Q2. Task B: Correlation\n",
    "Statisticians and data analysts usually like to study the correlation between different observed variables. This helps uncover interesting patterns in the data such as causal relationships (e.g., snow on the road leads to an increase in the number of accidents). Correlation studies are important for multiple reasons:\n",
    "- While [correlation does not imply causation](https://en.wikipedia.org/wiki/Correlation_does_not_imply_causation), a lack of correlation implies a lack of causation. This can be used to rule out many causal relationships.\n",
    "- Correlation helps with prediction. The more closely related two variables are, the easier it is to predict one from the other.\n",
    "\n",
    "In this task, we ask you to plot the friends_count (on y-axis) vs the followers_count (on x-axis) using the matplotlib package. [Here](http://matplotlib.org/examples/shapes_and_collections/scatter_demo.html) is an example to get started with scatter plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hm8m3vZ6S3YB"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_friends_vs_followers(users_df):\n",
    "    \"\"\"Plots friends_count vs. followers_count using a scatter plot.\"\"\"\n",
    "    # Scatter plot\n",
    "    scatter = plt.scatter(users_df['followers_count'], users_df['friends_count'], alpha=0.6, edgecolors='w', s=50)\n",
    "\n",
    "    # Labels and title\n",
    "    plt.xlabel(\"Followers Count\")\n",
    "    plt.ylabel(\"Friends Count\")\n",
    "    plt.title(\"Scatter Plot of Friends Count vs. Followers Count\")\n",
    "\n",
    "    # Display grid for better visualization\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "    # Show plot\n",
    "    plt.show()\n",
    "    return scatter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ziJiLUeS3YD"
   },
   "source": [
    "Do you see a correlation between these two variables from your scatter plot? Let's measure this quantitatively using the [Pearson's correlation coefficient](https://en.wikipedia.org/wiki/Pearson_product-moment_correlation_coefficient). \n",
    "\n",
    "For a set of observations $(X,Y) = [(x_1,y_1), (x_2,y_2), ... , (x_n,y_n)]$, the Pearson's correlation coefficient is a measure of the linear dependence between two variables $X$ and $Y$, giving a value between +1 and −1 inclusive, where 1 is total positive correlation, 0 is no correlation, and −1 is total negative correlation.\n",
    "\n",
    "$r=r_{xy}={\\frac {n\\sum x_{i}y_{i}-\\sum x_{i}\\sum y_{i}}{{\\sqrt {n\\sum x_{i}^{2}-(\\sum x_{i})^{2}}}~{\\sqrt {n\\sum y_{i}^{2}-(\\sum y_{i})^{2}}}}}$\n",
    "\n",
    "Now, fill in the following function to compute the Pearson's correlation coefficient between friends_count and followers_count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nRfkg_jjS3YD"
   },
   "outputs": [],
   "source": [
    "def correlation_coefficient(users_df):\n",
    "    \"\"\"Computes Pearson's correlation coefficient between friends_count and followers_count.\"\"\"\n",
    "    # Extract variables\n",
    "    x = users_df['followers_count']\n",
    "    y = users_df['friends_count']\n",
    "\n",
    "    # Compute terms for Pearson's formula\n",
    "    n = len(users_df)\n",
    "    sum_x = x.sum()\n",
    "    sum_y = y.sum()\n",
    "    sum_xy = (x * y).sum()\n",
    "    sum_x2 = (x ** 2).sum()\n",
    "    sum_y2 = (y ** 2).sum()\n",
    "\n",
    "    # Compute Pearson's correlation coefficient\n",
    "    numerator = n * sum_xy - sum_x * sum_y\n",
    "    denominator = ((n * sum_x2 - sum_x**2) * (n * sum_y2 - sum_y**2))**0.5\n",
    "\n",
    "    # Return correlation coefficient\n",
    "    return numerator / denominator if denominator != 0 else 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8YsJPP3FS3YE"
   },
   "source": [
    "### Q2. Task C: Degree distribution\n",
    "If you are not familiar with graph theory and/or graph mining, skip the first paragraph.\n",
    "\n",
    "As you're familiar with graphs, you might know that the degree of a node is the number of connections it has to other nodes. A common statistic to look out for in the case of real-world graphs is the degree distribution. Literature says degrees of nodes in real-world graphs follow a [power law distribution](https://en.wikipedia.org/wiki/Power_law). The implication is that a scatter plot of num_users versus k (as we will define below) yields an almost straight line. In this task, we shall verify whether the given crawl of the Twitter network satisfies this property.\n",
    "\n",
    "Let us call the number of friends a Twitter user has as his/her degree. The degree distribution is a histogram of the number of friends. Your task is to visualize this histogram. Use the default number of bins.\n",
    "\n",
    "Do you notice any surprising/unexpected patterns? What can you say about the way in which the Twitter data was collected?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CtgPbYjjS3YE"
   },
   "outputs": [],
   "source": [
    "def degree_distribution(edges_df):\n",
    "    \"\"\"Plots the distribution of friends (degree) for users.\"\"\"\n",
    "    # Calculate degree (number of friends for each user)\n",
    "    degree_counts = edges_df['from_user'].value_counts()\n",
    "\n",
    "    # Plot the histogram\n",
    "    hist_values = plt.hist(degree_counts, bins='auto', color='blue', alpha=0.7, edgecolor='black')\n",
    "\n",
    "    # Labels and title\n",
    "    plt.xlabel(\"Number of Friends (Degree)\")\n",
    "    plt.ylabel(\"Number of Users\")\n",
    "    plt.title(\"Degree Distribution of Twitter Users\")\n",
    "\n",
    "    # Log scale (optional, for power-law verification)\n",
    "    plt.yscale(\"log\")\n",
    "    plt.xscale(\"log\")\n",
    "\n",
    "    # Show plot\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.show()\n",
    "\n",
    "    return hist_values\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
