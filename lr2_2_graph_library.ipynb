{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wSkzSOckYIei"
   },
   "outputs": [],
   "source": [
    "# Colab only: Run this cell to download/install packages\n",
    "import sysu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wOJpy7dfYGkO"
   },
   "source": [
    "# Graph Library\n",
    "\n",
    "\n",
    "In this problem, you're going to write a (very minimal) graph library, which uses both the adjacency dictionary and the sparse adjacency matrix representation of a graph.  Using these two representations, you'll implement two of the more well-known large-scale graph processing algorithms: Dijkstra's algorithm for finding single-source shortest paths in the graph, and the PageRank algorithm for determining the importance of nodes in the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tR6_ZvMEYGkT"
   },
   "source": [
    "### The `wikipedia_small` graph\n",
    "\n",
    "The graph we'll be focusing on for most of this assignment is a directed graph that represents the page links in the English language Wikipedia.  Specifically, we took the (pre-processed) Wikipedia dump from `http://haselgrove.id.au/wikipedia.htm`, which were taken from a 2008 version of Wikipedia, and we then subselected only those nodes that had at least _500 incoming links_.  This resulted in a graph with about 24,000 nodes and 6,000,000 edges.\n",
    "\n",
    "*Note*: The `http://haselgrove.id.au/wikipedia.htm` has since died off. See Wikipedia itself for information on accessing data: https://en.wikipedia.org/wiki/Wikipedia:Database_download#Where_do_I_get_it?\n",
    "\n",
    "There are two files included with this notebook that are relevant here:\n",
    "\n",
    "- `wikipedia_small.graph.gz`\n",
    "- `wikipedia_small.nodes.gz`\n",
    "    \n",
    "The `.graph.gz` file contains a (gzipped) list of integers, two per line.  If the line \"`i j`\" appears in the file, this indicates a directed edge from node `i` to node `j`.  The `.nodes.gz` file then contains a (gzipped) list of each node the graph, where the link number indicates the node index.  This is how we can relate the node numbers in the `.graph.gz` file to actual pages on Wikipedia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cc4NVb6QYGkU"
   },
   "source": [
    "## Your own Graph class\n",
    "\n",
    "In the main portion of this assignment, you'll create your own Graph class that mimics some of the functionality of networkx. We'll provide you with some scaffolding and support code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1665480383229,
     "user": {
      "displayName": "Галина Василівна Мельник",
      "userId": "15364269542359895576"
     },
     "user_tz": -180
    },
    "id": "rAAneQF4YGkU"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import heapdict\n",
    "import gzip\n",
    "\n",
    "# Utility function to read the graph\n",
    "def read_graph(basename=\"wikipedia_small\"):\n",
    "    with gzip.open(f\"{basename}.nodes.gz\", 'rt', encoding=\"utf-8\") as f:\n",
    "        nodes = [a.strip() for a in f]\n",
    "    with gzip.open(f\"{basename}.graph.gz\", 'rt', encoding=\"utf-8\") as f:\n",
    "        links = []\n",
    "        for row in f:\n",
    "            i, j = tuple(row.strip().split())\n",
    "            links.append((nodes[int(i)], nodes[int(j)]))\n",
    "    return links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y0rzY2LoYGkW"
   },
   "source": [
    "### Q1: Implement `add_edges`\n",
    "\n",
    "Here is the template for your Graph class.\n",
    "\n",
    "Note that `self.edges` should be represented as an \"adjacency dictionary\", so that for every node `i` in the graph `self.edges[i]` is a dictionary of nodes that `i` is directly connected to. The value of the inner dictionary should be `1` for every edge that exists (the value of this entry doesn't matter, so we could technically use a dictionary of sets, but we use a dictionary of dictionaries to keep things a little bit more uniform and to allow for potential extensions e.g. to weighted graphs.)\n",
    "\n",
    "Note that all vertices must exist in the dictionary. If a vertex has no outgoing edges, then it should still have an entry pointing to an empty dictionary. \n",
    "\n",
    "To begin, implement the function `add_edges()`. It must modify the `self.edges` variable to add all edges passed as tuples in `edges_list`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7RJnynaxYGkX"
   },
   "outputs": [],
   "source": [
    "class Graph:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize with an empty edge dictionary.\"\"\"\n",
    "        self.edges = {}\n",
    "\n",
    "    def add_edges(self, edges_list):\n",
    "        \"\"\"\n",
    "        Add a list of edges to the network. Use 1 to indicate the presence of an edge.\n",
    "        \n",
    "        Args:\n",
    "            edges_list: list of (a, b) tuples, where a -> b is an edge to add.\n",
    "        \n",
    "        Returns:\n",
    "            self: instance of class (needed for testing).\n",
    "        \"\"\"\n",
    "        for a, b in edges_list:\n",
    "            if a not in self.edges:\n",
    "                self.edges[a] = {}\n",
    "            if b not in self.edges:\n",
    "                self.edges[b] = {}  # Ensure all nodes are represented\n",
    "            self.edges[a][b] = 1  # Indicate presence of an edge\n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ee9gb-kBYGkc"
   },
   "source": [
    "## Q2: Dijkstra's algorithm\n",
    "\n",
    "Next, implement Dijkstra's single-source shortest path algorithm (with the simple case where the distance along any edge is assumed to be one).  You can refer to the [Wikipedia page on Dijkstra's algorithm](https://en.wikipedia.org/wiki/Dijkstra%27s_algorithm) for reference. The basic idea of the algorithm is to keep a priority queue of nodes ordered by distance from a source node.  At each step, we continually pop off the smallest element `i` in the queue, and update the distance of all successor nodes to have a distance of `1 + distance[i]`.\n",
    "\n",
    "For the priority queue, you should use a [`heapdict`](https://github.com/DanielStutzbach/heapdict), which is a form of priority queue that allows you to change the priority of an element.\n",
    "\n",
    "When called with source node `A`, the function should return a dictionary where the keys are all the nodes in the graph. For each key `B`:\n",
    "\n",
    "- if `B` is reachable from `A` then the value must be the tuple `(distance, prev_node)`, where:\n",
    "  - `distance` is the minimum number of hops from `A` to `B`, and\n",
    "  - `prev_node` is the node immediately before `B` along one such shortest path\n",
    "- if `B` is not reachable from `A`, then the value must be `(inf,None)`\n",
    "- if `B == A`, then the value should be `(0, None)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m75c59CgYGkc"
   },
   "outputs": [],
   "source": [
    "def shortest_path(g, source):\n",
    "    \"\"\"\n",
    "    Compute the single-source shortest path using Dijkstra's algorithm.\n",
    "    \n",
    "    Args:\n",
    "        g: Graph object\n",
    "        source: source node\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary of node: (distance, prev_node) values for each node in the graph.\n",
    "        - If a node is unreachable, the value is (inf, None).\n",
    "        - For the source node itself, the value is (0, None).\n",
    "    \"\"\"\n",
    "    # Initialize distances and predecessors\n",
    "    distances = {node: (math.inf, None) for node in g.edges}\n",
    "    distances[source] = (0, None)  # Distance to source is 0\n",
    "    \n",
    "    # Priority queue\n",
    "    pq = heapdict()\n",
    "    for node in g.edges:\n",
    "        pq[node] = distances[node][0]  # Use the current distance as priority\n",
    "\n",
    "    while pq:\n",
    "        current_node, current_distance = pq.popitem()\n",
    "        \n",
    "        # Iterate through neighbors of the current node\n",
    "        for neighbor in g.edges[current_node]:\n",
    "            new_distance = current_distance + 1  # Edge weights are 1\n",
    "            if new_distance < distances[neighbor][0]:  # Found a shorter path\n",
    "                distances[neighbor] = (new_distance, current_node)\n",
    "                pq[neighbor] = new_distance  # Update priority queue\n",
    "\n",
    "    return distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T8vwWE1MYGke"
   },
   "source": [
    "## Q3: Adjacency matrix representation\n",
    "\n",
    "Implement the adjacency matrix method of the Graph class.  This returns a matrix representing the adjacency of the graph (in scipy COO sparse format), as well as a list of nodes that indicate how the indices in this graph relate to the nodes in the network.\n",
    "\n",
    "In order to complete this question in a manner that works on the Wikipedia graph, you must implement this function natively as a sparse matrix (i.e., you cannot construct a dense matrix and then convert that to a sparse matrix, but need to directly use the `sp.coo_matrix()` constructor).  The Wikipedia graph is 24K x 24K nodes, which (assuming 8 bytes per entry, would take up 4GB of memory.  While it's not impossible to do things this way at this scale (it quickly becomes infeasible for graphs that are even slightly larger), it's a very bad idea, and just allocating this much memory will take too long.\n",
    "\n",
    "**Note the order of the axes of the output matrix.** This is important for calculating the PageRank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-PzHyl5jYGkf"
   },
   "outputs": [],
   "source": [
    "def adjacency_matrix(g):\n",
    "    \"\"\"\n",
    "    Compute an adjacency matrix form of the graph.\n",
    "    \n",
    "    Args:\n",
    "        g: Graph object.\n",
    "        \n",
    "    Returns: tuple (A, nodes)\n",
    "        A: a sparse matrix in COO form that represents the adjacency matrix\n",
    "           for the graph (i.e., A[j,i] = 1 iff there is an edge i->j)\n",
    "           NOTE: Be sure you have this ordering correct!\n",
    "        nodes: a list of nodes indicating the node key corresponding to each\n",
    "               index of the A matrix.\n",
    "    \"\"\"\n",
    "    # List of nodes in sorted order (to maintain a consistent mapping)\n",
    "    nodes = sorted(g.edges.keys())\n",
    "    node_index = {node: i for i, node in enumerate(nodes)}  # Map node to index\n",
    "\n",
    "    # Prepare lists to construct the sparse matrix\n",
    "    row_indices = []\n",
    "    col_indices = []\n",
    "    data = []\n",
    "\n",
    "    # Populate the adjacency information\n",
    "    for node, neighbors in g.edges.items():\n",
    "        for neighbor in neighbors:\n",
    "            col_indices.append(node_index[node])  # From-node\n",
    "            row_indices.append(node_index[neighbor])  # To-node\n",
    "            data.append(1)  # Presence of edge\n",
    "\n",
    "    # Create the sparse adjacency matrix in COO format\n",
    "    A = sp.coo_matrix((data, (row_indices, col_indices)), shape=(len(nodes), len(nodes)))\n",
    "\n",
    "    return A, nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "00lNvlcnYGkh"
   },
   "source": [
    "Make sure your code works on the Wikipedia graph.  In our implementation, it takes about 4 seconds to generate this matrix from the Wikipedia graph (not including the time of the `read_graph` function)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uY5jcYyGYGki"
   },
   "source": [
    "## Q4: PageRank algorithm\n",
    "\n",
    "Finally, implement the PageRank algorithm using the adjacency matrix representation. You should use the approach described in the [\"Power method\" section on the Wikipedia entry](https://en.wikipedia.org/wiki/PageRank#Power_method), which we also discussed in class.\n",
    "\n",
    "This involves forming some initial uniform probability vector $x$, and repeatly multiplying it by the matrices:\n",
    "\\begin{equation}\n",
    "x \\gets \\left(d P + \\left(1-d\\right)\\frac{1}{n} E \\right)x\n",
    "\\end{equation}\n",
    "where $P$ is a transition matrix, $E$ is the matrix of all ones, and $d$ is the damping factor, and $n$ is the number of nodes. You get $P$ by normalizing $A$ so that all columns have sum 1.\n",
    "\n",
    "*Note*: Here we are defining $d$ to be the \"damping factor\", which means we will randomly restart with probability $(1-d)$.\n",
    "\n",
    "Recall that from the definition of PageRank, when we reach a \"sink\" node (a node with no outgoing edges), we randomly hop to any other node in the network, so that columns of $P$ that have no outgoing edges are set to the uniform distribution.  To be efficient, you'll also want to avoid explicitly forming the $E$ matrix, and should instead use the fact that $E = \\mathbf{1}\\mathbf{1}^T$ where $\\mathbf{1}$ denotes a vector of all ones.  Use the fact that we can reorder matrix multiplication if associative (i.e., the fact the $A(BC)$ = $(AB)C$) to make this operation as fast as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sQAn-9raYGkj"
   },
   "source": [
    "Your function should return a dictionary of nodes and their corresponding page rank.  For example, in the five-node graph from our test cases, we have the following results:\n",
    "```\n",
    "C: 0.324\n",
    "B: 0.281\n",
    "E: 0.188\n",
    "D: 0.121\n",
    "A: 0.085\n",
    "```\n",
    "As is intuitive, nodes B and C have higher page rank, as they are pointed to by more of the other nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tIsmEYxGYGkk"
   },
   "outputs": [],
   "source": [
    "def pagerank(g, d=0.85, iters=100):\n",
    "    \"\"\"\n",
    "    Compute the PageRank score for each node in the network using the power method.\n",
    "    \n",
    "    Args:\n",
    "        g: Graph object.\n",
    "        d: PageRank damping factor (restart with probability (1-d)).\n",
    "        iters: Number of iterations to run.\n",
    "        \n",
    "    Returns:\n",
    "        ranks: A dictionary of node:importance score for each node in the network \n",
    "               (larger score means higher rank).\n",
    "    \"\"\"\n",
    "    # Step 1: Compute adjacency matrix and list of nodes\n",
    "    A, nodes = adjacency_matrix(g)\n",
    "    n = len(nodes)  # Number of nodes\n",
    "    node_index = {node: i for i, node in enumerate(nodes)}  # Map nodes to indices\n",
    "\n",
    "    # Step 2: Normalize columns of A to create the transition matrix P\n",
    "    out_degree = np.array(A.sum(axis=0)).flatten()  # Sum of columns (out-degrees)\n",
    "    out_degree[out_degree == 0] = 1  # Avoid division by zero for sink nodes\n",
    "    P = sp.coo_matrix(A / out_degree)\n",
    "\n",
    "    # Step 3: Initialize the PageRank vector x with uniform probabilities\n",
    "    x = np.ones(n) / n\n",
    "\n",
    "    # Step 4: Compute PageRank using the power method\n",
    "    for _ in range(iters):\n",
    "        # Apply the PageRank update rule\n",
    "        x = d * P.dot(x) + (1 - d) / n\n",
    "\n",
    "    # Step 5: Map the final scores back to the nodes\n",
    "    ranks = {nodes[i]: x[i] for i in range(n)}\n",
    "\n",
    "    return ranks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fTXr0ChIYGkm"
   },
   "source": [
    "Make sure your implementation works on the full Wikipedia graph (in our implementation, it takes 11 seconds to run, most of which is taken up by generating the adjacency matrix).  The top PageRank entires we get from our implementation are:\n",
    "\n",
    "```\n",
    "United_States     2.75e-3\n",
    "2007              2.44e-3\n",
    "2008              2.17e-3\n",
    "Wikimedia_Commons 1.72e-3\n",
    "United_Kingdom    1.59e-3\n",
    "2006              1.54e-3\n",
    "France            1.44e-3\n",
    "Wiktionary        1.26e-3\n",
    "Canada            1.09e-3\n",
    "World_War_II      1.04e-3\n",
    "2005              1.04e-3\n",
    "List_of_Africa... 1.00e-3\n",
    "Germany           0.95e-3\n",
    "Europe            0.93e-3\n",
    "English_language  0.90e-3\n",
    "Geographic_coo... 0.89e-3\n",
    "Latin             0.88e-3\n",
    "Australia         0.87e-3\n",
    "India             0.78e-3\n",
    "Japan             0.78e-3\n",
    "```\n",
    "\n",
    "countries and years! A seemingly reasonable list of pages we may expect to be important."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
